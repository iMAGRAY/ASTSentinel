{"session_id":"506f332d-a175-4459-b886-985f0c0d6313","transcript_path":"C:\\Users\\1\\.claude\\projects\\C--Users-1-Documents-GitHub-ValidationCodeHook\\506f332d-a175-4459-b886-985f0c0d6313.jsonl","cwd":"C:\\Users\\1\\Documents\\GitHub\\ValidationCodeHook","permission_mode":"bypassPermissions","hook_event_name":"PostToolUse","tool_name":"Edit","tool_input":{"file_path":"src/analysis/duplicate_detector.rs","old_string":"#[derive(Debug, PartialEq)]\npub enum ConflictType {","new_string":"#[derive(Debug, PartialEq, Eq, Hash)]\npub enum ConflictType {"},"tool_response":{"filePath":"src/analysis/duplicate_detector.rs","oldString":"#[derive(Debug, PartialEq)]\npub enum ConflictType {","newString":"#[derive(Debug, PartialEq, Eq, Hash)]\npub enum ConflictType {","originalFile":"use sha2::{Digest, Sha256};\nuse std::collections::HashMap;\nuse std::fs;\nuse std::path::{Path, PathBuf};\n\n#[derive(Debug, Clone)]\npub struct FileInfo {\n    pub path: PathBuf,\n    pub size: u64,\n    pub hash: String,\n    pub lines: usize,\n    pub modified: std::time::SystemTime,\n}\n\n#[derive(Debug)]\npub struct DuplicateGroup {\n    pub pattern: String,\n    pub files: Vec<FileInfo>,\n    pub conflict_type: ConflictType,\n}\n\n#[derive(Debug, PartialEq)]\npub enum ConflictType {\n    ExactDuplicate,  // Same content, different names\n    SimilarName,     // test.js, test2.js, test_old.js\n    BackupFile,      // .bak, .old, .backup, ~\n    VersionConflict, // v1, v2, _new, _old\n    TempFile,        // .tmp, .temp, .swp\n}\n\npub struct DuplicateDetector {\n    files: Vec<FileInfo>,\n}\n\nimpl Default for DuplicateDetector {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl DuplicateDetector {\n    pub fn new() -> Self {\n        Self { files: Vec::new() }\n    }\n\n    pub fn scan_directory(&mut self, dir: &Path) -> Result<(), Box<dyn std::error::Error>> {\n        self.scan_recursive(dir, 0)?;\n        Ok(())\n    }\n\n    fn scan_recursive(\n        &mut self,\n        dir: &Path,\n        depth: usize,\n    ) -> Result<(), Box<dyn std::error::Error>> {\n        if depth > 10 {\n            return Ok(());\n        } // Prevent infinite recursion\n\n        let entries = fs::read_dir(dir)?;\n        for entry in entries {\n            let entry = entry?;\n            let path = entry.path();\n\n            // Skip common non-source directories\n            if let Some(name) = path.file_name() {\n                let name_str = name.to_string_lossy();\n                if name_str.starts_with('.')\n                    || name_str == \"node_modules\"\n                    || name_str == \"target\"\n                    || name_str == \"dist\"\n                    || name_str == \"build\"\n                {\n                    continue;\n                }\n            }\n\n            if path.is_file() {\n                if let Ok(metadata) = entry.metadata() {\n                    if metadata.len() > 0 && metadata.len() < 10_000_000 {\n                        // Skip huge files\n                        if let Ok(content) = fs::read(&path) {\n                            let hash = format!(\"{:x}\", Sha256::digest(&content));\n\n                            // Correct line counting - convert to string and count lines properly\n                            let lines = if let Ok(content_str) = std::str::from_utf8(&content) {\n                                if content_str.is_empty() {\n                                    0\n                                } else {\n                                    content_str.lines().count()\n                                }\n                            } else {\n                                // Fallback for binary files - count \\n bytes\n                                content.iter().filter(|&&b| b == b'\\n').count()\n                            };\n\n                            // Better error handling for modified time\n                            let modified = metadata.modified().unwrap_or_else(|e| {\n                                eprintln!(\n                                    \"Warning: Failed to get modified time for {}: {}\",\n                                    path.display(),\n                                    e\n                                );\n                                std::time::UNIX_EPOCH\n                            });\n\n                            self.files.push(FileInfo {\n                                path: path.clone(),\n                                size: metadata.len(),\n                                hash,\n                                lines,\n                                modified,\n                            });\n                        } else {\n                            eprintln!(\"Warning: Failed to read file content: {}\", path.display());\n                        }\n                    }\n                }\n            } else if path.is_dir() {\n                self.scan_recursive(&path, depth + 1)?;\n            }\n        }\n        Ok(())\n    }\n\n    pub fn find_duplicates(&self) -> Vec<DuplicateGroup> {\n        let mut groups = Vec::new();\n\n        // 1. Find exact content duplicates\n        let mut hash_map: HashMap<String, Vec<&FileInfo>> = HashMap::new();\n        for file in &self.files {\n            hash_map\n                .entry(file.hash.clone())\n                .or_default()\n                .push(file);\n        }\n\n        for (hash, files) in hash_map {\n            if files.len() > 1 {\n                groups.push(DuplicateGroup {\n                    pattern: format!(\"Content hash: {}\", &hash[..8]),\n                    files: files.into_iter().cloned().collect(),\n                    conflict_type: ConflictType::ExactDuplicate,\n                });\n            }\n        }\n\n        // 2. Find similar named files (potential versions)\n        let mut name_groups: HashMap<String, Vec<&FileInfo>> = HashMap::new();\n        for file in &self.files {\n            if let Some(stem) = file.path.file_stem() {\n                let stem_str = stem.to_string_lossy().to_lowercase();\n\n                // Skip standard Rust module files - these are expected to exist in multiple directories\n                if Self::is_standard_filename(&stem_str) {\n                    continue;\n                }\n\n                let clean_stem = Self::clean_filename(&stem_str);\n\n                // Include parent directory in the grouping key to avoid false positives\n                // for files with same name in different directories serving different purposes\n                let parent_dir = file\n                    .path\n                    .parent()\n                    .and_then(|p| p.file_name())\n                    .map(|p| p.to_string_lossy().to_lowercase())\n                    .unwrap_or_else(|| \"root\".to_string());\n\n                let grouping_key = format!(\"{}::{}\", parent_dir, clean_stem);\n\n                name_groups.entry(grouping_key).or_default().push(file);\n            }\n        }\n\n        for (pattern, files) in name_groups {\n            if files.len() > 1 {\n                // Check if they're actually different versions\n                let unique_hashes: std::collections::HashSet<_> =\n                    files.iter().map(|f| &f.hash).collect();\n\n                if unique_hashes.len() > 1 {\n                    let conflict_type = Self::detect_conflict_type(&files);\n                    groups.push(DuplicateGroup {\n                        pattern: pattern.clone(),\n                        files: files.into_iter().cloned().collect(),\n                        conflict_type,\n                    });\n                }\n            }\n        }\n\n        // Deterministic ordering: by conflict type priority, then pattern asc\n        fn prio(t: &ConflictType) -> u8 {\n            match t {\n                ConflictType::ExactDuplicate => 0,\n                ConflictType::VersionConflict => 1,\n                ConflictType::BackupFile => 2,\n                ConflictType::TempFile => 3,\n                ConflictType::SimilarName => 4,\n            }\n        }\n\n        let sum = |g: &DuplicateGroup| -> u64 { g.files.iter().map(|f| f.size).sum() };\n\n        groups.sort_by(|a, b| prio(&a.conflict_type)\n            .cmp(&prio(&b.conflict_type))\n            .then_with(|| sum(b).cmp(&sum(a))) // larger groups first within same type\n            .then_with(|| a.pattern.cmp(&b.pattern)));\n        groups\n    }\n\n    /// Check if a filename is a standard file that's expected to exist in multiple directories\n    fn is_standard_filename(name: &str) -> bool {\n        matches!(\n            name,\n            \"mod\" |           // Rust module files\n            \"lib\" |           // Library root files\n            \"main\" |          // Main entry point files\n            \"index\" |         // Index files (common in web projects)\n            \"readme\" |        // README files\n            \"__init__\" |      // Python package files\n            \"makefile\" |      // Build files\n            \"dockerfile\" |    // Docker files\n            \"package\" |       // Package manifest files (package.json, etc.)\n            \"cargo\" |         // Cargo manifest files\n            \"pyproject\" |     // Python project files\n            \"setup\" |         // Setup/configuration files\n            \"config\" |        // Configuration files\n            \"test\" |          // Generic test files\n            \"tests\" |         // Test suite files\n            \"spec\" // Specification files\n        )\n    }\n\n    fn clean_filename(name: &str) -> String {\n        // Only remove clear version/backup indicators, preserve meaningful name parts\n        // Don't strip numbers or 'v' indiscriminately as they may be part of the actual name\n        let cleaned = name\n            .replace(\"_old\", \"\")\n            .replace(\"_new\", \"\")\n            .replace(\"_backup\", \"\")\n            .replace(\"_copy\", \"\")\n            .replace(\"_temp\", \"\")\n            .replace(\"_tmp\", \"\")\n            .replace(\".backup\", \"\")\n            .replace(\".old\", \"\")\n            .replace(\".bak\", \"\")\n            .replace(\"~\", \"\");\n\n        // Only trim underscores/dashes if they're at edges\n        cleaned.trim_matches('_').trim_matches('-').to_string()\n    }\n\n    fn detect_conflict_type(files: &[&FileInfo]) -> ConflictType {\n        let names: Vec<String> = files\n            .iter()\n            .filter_map(|f| f.path.file_name())\n            .map(|n| n.to_string_lossy().to_lowercase())\n            .collect();\n\n        // Check for backup patterns\n        if names.iter().any(|n| {\n            n.contains(\".bak\") || n.contains(\".old\") || n.contains(\"backup\") || n.ends_with('~')\n        }) {\n            return ConflictType::BackupFile;\n        }\n\n        // Check for temp files\n        if names\n            .iter()\n            .any(|n| n.contains(\".tmp\") || n.contains(\".temp\") || n.contains(\".swp\"))\n        {\n            return ConflictType::TempFile;\n        }\n\n        // Check for version patterns\n        if names.iter().any(|n| {\n            n.contains(\"_v\") || n.contains(\"_new\") || n.contains(\"_old\") || n.contains(\"copy\")\n        }) {\n            return ConflictType::VersionConflict;\n        }\n\n        ConflictType::SimilarName\n    }\n\n    pub fn format_report(&self, groups: &[DuplicateGroup]) -> String {\n        if groups.is_empty() {\n            return String::new();\n        }\n\n        // Soft caps to keep report compact\n        let max_groups: usize = std::env::var(\"DUP_REPORT_MAX_GROUPS\").ok().and_then(|v| v.parse().ok()).unwrap_or(20).clamp(1, 200);\n        let max_files: usize = std::env::var(\"DUP_REPORT_MAX_FILES\").ok().and_then(|v| v.parse().ok()).unwrap_or(10).clamp(1, 200);\n\n        let mut report = String::from(\"\\n🔴 **КРИТИЧНО: Обнаружены дубликаты/конфликты файлов**\\n\");\n\n        // Summary per conflict type\n        let mut counts: std::collections::HashMap<ConflictType, usize> = std::collections::HashMap::new();\n        for g in groups { *counts.entry(g.conflict_type.clone()).or_insert(0) += 1; }\n        report.push_str(\"Сводка по типам: \");\n        let mut parts = Vec::new();\n        let pushp = |t: ConflictType, name: &str, v: &mut Vec<String>| {\n            if let Some(c) = counts.get(&t) { v.push(format!(\"{}:{}\", name, c)); }\n        };\n        pushp(ConflictType::ExactDuplicate, \"Exact\", &mut parts);\n        pushp(ConflictType::VersionConflict, \"Version\", &mut parts);\n        pushp(ConflictType::BackupFile, \"Backup\", &mut parts);\n        pushp(ConflictType::TempFile, \"Temp\", &mut parts);\n        pushp(ConflictType::SimilarName, \"Similar\", &mut parts);\n        report.push_str(&parts.join(\", \"));\n        report.push('\\n');\n\n        let shown_groups = groups.iter().take(max_groups);\n        let hidden_groups = groups.len().saturating_sub(max_groups);\n\n        for group in shown_groups {\n            let conflict_icon = match group.conflict_type {\n                ConflictType::ExactDuplicate => \"🔁\",\n                ConflictType::BackupFile => \"💾\",\n                ConflictType::TempFile => \"🗑️\",\n                ConflictType::VersionConflict => \"⚠️\",\n                ConflictType::SimilarName => \"📝\",\n            };\n\n            report.push_str(&format!(\n                \"\\n{} **{:?}** ({})\\n\",\n                conflict_icon, group.conflict_type, group.pattern\n            ));\n\n            // Sort files by size (largest first) and modification time\n            let mut sorted_files = group.files.clone();\n            sorted_files.sort_by(|a, b| {\n                b.size\n                    .cmp(&a.size)\n                    .then_with(|| b.modified.cmp(&a.modified))\n            });\n\n            if sorted_files.len() > max_files { sorted_files.truncate(max_files); }\n            let hidden_files = group.files.len().saturating_sub(max_files);\n\n            for (i, file) in sorted_files.iter().enumerate() {\n                let path_str = file.path.display().to_string();\n                let relative_path = path_str\n                    .split(\"ValidationCodeHook\")\n                    .last()\n                    .or_else(|| path_str.split(\"GitHub\").last())\n                    .unwrap_or(&path_str);\n\n                let is_likely_main = i == 0; // Largest and newest is likely the main one\n                let marker = if is_likely_main {\n                    \"→ ОСНОВНОЙ\"\n                } else {\n                    \"  \"\n                };\n\n                report.push_str(&format!(\n                    \"  {} {} | {}B | {}L | {}\\n\",\n                    marker,\n                    relative_path,\n                    file.size,\n                    file.lines,\n                    &file.hash[..8]\n                ));\n            }\n\n            if hidden_files > 0 {\n                report.push_str(&format!(\"  ... и ещё {} файлов скрыто по лимиту\\n\", hidden_files));\n            }\n\n            // Add recommendation\n            if group.conflict_type == ConflictType::ExactDuplicate {\n                report.push_str(\"  💡 Удалить дубликаты, оставить один файл\\n\");\n            } else if group.conflict_type == ConflictType::BackupFile\n                || group.conflict_type == ConflictType::TempFile\n            {\n                report.push_str(\"  💡 Удалить backup/temp файлы\\n\");\n            } else {\n                report.push_str(\"  💡 Объединить изменения в один файл\\n\");\n            }\n        }\n\n        if hidden_groups > 0 {\n            report.push_str(&format!(\"\\n... и ещё {} групп скрыто по лимиту\\n\", hidden_groups));\n        }\n\n        report\n    }\n}\n","structuredPatch":[{"oldStart":19,"oldLines":7,"newStart":19,"newLines":7,"lines":["     pub conflict_type: ConflictType,"," }"," ","-#[derive(Debug, PartialEq)]","+#[derive(Debug, PartialEq, Eq, Hash)]"," pub enum ConflictType {","     ExactDuplicate,  // Same content, different names","     SimilarName,     // test.js, test2.js, test_old.js"]}],"userModified":false,"replaceAll":false}}